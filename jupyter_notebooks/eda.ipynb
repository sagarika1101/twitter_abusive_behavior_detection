{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ramya\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tqdm\\std.py:666: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/hatespeech_text_label_vote_RESTRICTED_100K.csv', header=None, sep='\\t')\n",
    "df.columns = ['text', 'label', 'votes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abusive</td>\n",
       "      <td>27150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hateful</td>\n",
       "      <td>4965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>normal</td>\n",
       "      <td>53851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spam</td>\n",
       "      <td>14030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     label  counts\n",
       "0  abusive   27150\n",
       "1  hateful    4965\n",
       "2   normal   53851\n",
       "3     spam   14030"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['label']).size().reset_index(name='counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    text = re.sub(r'\\w*@\\w*\\.\\w*', '', text)  # remove emails\n",
    "    text = re.sub(r'#.+?(\\s|$)', '', text)  # remove hashtags\n",
    "    text = re.sub(r'(https{0,1}\\:\\/\\/.+?(\\s|$))|(www\\..+?(\\s|$))|('\n",
    "                  r'\\b\\w+\\.twitter\\.com.+?(\\s|$))', ' ',\n",
    "                  text)  # remove urls\n",
    "    text = re.sub(r'(@.+?(\\b|\\s|$))', '', text)  # remove mentions\n",
    "    text = re.sub(r'\\b(RT|rt)\\b', '', text)  # remove retweets\n",
    "    text = re.sub(r'(&#\\w+;)+', ' ', text)  # &amp;\n",
    "    text = re.sub(r'(&amp;)+', ' ', text)\n",
    "    text = re.sub(r'(\\\\n)+', ' ', text)  # remove \\n\n",
    "    text = re.sub(r'(\\[deleted\\])|(\\[removed\\])', ' ',\n",
    "                  text)  # remove [deleted] and [removed]\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "def lemmatize(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    return \" \".join([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 99996/99996 [00:04<00:00, 24159.87it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 99996/99996 [12:19<00:00, 135.14it/s]\n"
     ]
    }
   ],
   "source": [
    "df['cleaned'] = df['text'].progress_apply(lambda x : clean(x))\n",
    "df['lemmatized'] = df['cleaned'].progress_apply(lambda x: lemmatize(x))\n",
    "df.to_pickle('data/processed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text          Beats by Dr. Dre urBeats Wired In-Ear Headphones - White https://t.co/9tREpqfyW4 https://t.co/FCaWyWRbpE\n",
       "label         1                                                                                                       \n",
       "votes         4                                                                                                       \n",
       "cleaned       beats by dr. dre urbeats wired in-ear headphones - white                                                \n",
       "lemmatized    beat by dr . dre urbeats wire in - ear headphone - white                                                \n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/processed.pkl', 'rb') as f:\n",
    "    df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_label(x):\n",
    "    if x == 'normal':\n",
    "        return 0\n",
    "    elif x == 'spam':\n",
    "        return 1\n",
    "    elif x == 'abusive':\n",
    "        return 2\n",
    "    else:\n",
    "        return 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df['label'].apply(lambda x : assign_label(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_vectorizer = CountVectorizer(input='content',\n",
    "                                   analyzer='word',\n",
    "                                   strip_accents='ascii',\n",
    "                                   ngram_range=(1,3),\n",
    "                                   min_df=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = ngram_vectorizer.fit_transform(train_df['lemmatized'])\n",
    "trainY = train_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = OneVsOneClassifier(svm.LinearSVC(class_weight='balanced', C=0.1, max_iter=2000), n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsOneClassifier(estimator=LinearSVC(C=0.1, class_weight='balanced',\n",
       "                                       dual=True, fit_intercept=True,\n",
       "                                       intercept_scaling=1,\n",
       "                                       loss='squared_hinge', max_iter=2000,\n",
       "                                       multi_class='ovr', penalty='l2',\n",
       "                                       random_state=None, tol=0.0001,\n",
       "                                       verbose=0),\n",
       "                   n_jobs=-1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(trainX, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "predY = classifier.predict(trainX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96     43003\n",
      "           1       0.78      0.97      0.86     11213\n",
      "           2       0.97      0.96      0.96     21781\n",
      "           3       0.82      0.97      0.89      3999\n",
      "\n",
      "    accuracy                           0.94     79996\n",
      "   macro avg       0.89      0.95      0.92     79996\n",
      "weighted avg       0.95      0.94      0.94     79996\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(trainY, predY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX = ngram_vectorizer.transform(test_df['lemmatized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "predY = classifier.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "testY = test_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.80      0.83     10848\n",
      "           1       0.49      0.62      0.54      2817\n",
      "           2       0.88      0.86      0.87      5369\n",
      "           3       0.40      0.39      0.40       966\n",
      "\n",
      "    accuracy                           0.77     20000\n",
      "   macro avg       0.65      0.67      0.66     20000\n",
      "weighted avg       0.79      0.77      0.78     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(testY, predY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.537564\n",
       "1    0.140170\n",
       "2    0.272276\n",
       "3    0.049990\n",
       "Name: counts, dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby('label').size().reset_index(name='counts')['counts']/79996"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.54240\n",
       "1    0.14085\n",
       "2    0.26845\n",
       "3    0.04830\n",
       "Name: counts, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.groupby('label').size().reset_index(name='counts')['counts']/20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 5)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vectorizer = CountVectorizer(input='content',\n",
    "                                 analyzer='word',\n",
    "                                 strip_accents='ascii',\n",
    "                                 ngram_range=(1,3),\n",
    "                                 stop_words='english',\n",
    "                                 min_df=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pickle.load(open('data/processed.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_abusive = df[df['label'] == 'abusive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanjana/anaconda3/envs/west/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/sanjana/anaconda3/envs/west/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "df_abusive['cleaned'] = df_abusive['text'].apply(lambda x : clean(x))\n",
    "df_abusive['lemmatized'] = df_abusive['cleaned'].apply(lambda x: lemmatize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vectorizer = CountVectorizer(input='content',\n",
    "                                 analyzer='word',\n",
    "                                 strip_accents='ascii',\n",
    "                                 ngram_range=(2,5),\n",
    "                                 stop_words='english',\n",
    "                                 min_df=10)\n",
    "X = bow_vectorizer.fit_transform(df_abusive['lemmatized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ugly ass',\n",
       " 'bad bitch',\n",
       " 'ass bitch',\n",
       " 'feel like',\n",
       " 'know fucking',\n",
       " 'bitch like',\n",
       " 'wanna fuck',\n",
       " 'just fucking',\n",
       " 'oh god',\n",
       " 'look like',\n",
       " 'stop fuck',\n",
       " 'fucking hate',\n",
       " 'fucking george',\n",
       " 'george bush',\n",
       " 'know fucking george',\n",
       " 'know fucking george bush racist',\n",
       " 'fucking george bush racist',\n",
       " 'fucking george bush',\n",
       " 'george bush racist',\n",
       " 'bush racist',\n",
       " 'know fucking george bush',\n",
       " 'sorry ugly',\n",
       " 'ugly ass bitch',\n",
       " 'sweetie sorry',\n",
       " 'sorry ugly ass',\n",
       " 'stupid ass',\n",
       " 'sorry ugly ass bitch',\n",
       " 'like say',\n",
       " 'damn phone',\n",
       " 'ass bitch like',\n",
       " 'look fucking',\n",
       " 'like fucking',\n",
       " 'just fuck',\n",
       " 'sorry ugly ass bitch like',\n",
       " 'say oh',\n",
       " 'ugly ass bitch like',\n",
       " 'bitch like say',\n",
       " 'say oh god',\n",
       " 'like say oh',\n",
       " 'know fuck',\n",
       " 'fucking bad',\n",
       " 'bitch like say oh',\n",
       " 'like say oh god',\n",
       " 'fucking thing',\n",
       " 'bitch like say oh god',\n",
       " 'ugly ass bitch like say',\n",
       " 'fucking hell',\n",
       " 'ass bitch like say',\n",
       " 'sick damn',\n",
       " 'say sick']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = np.array(bow_vectorizer.get_feature_names())\n",
    "feature_names[np.argsort(np.array(X.sum(axis=0))[0])[::-1]][:50].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(input='content',\n",
    "                                   analyzer='word',\n",
    "                                   strip_accents='ascii',\n",
    "                                   ngram_range=(1,3),\n",
    "                                   min_df=5,\n",
    "                                   stop_words='english',\n",
    "                                   use_idf = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = tfidf_vectorizer.fit_transform(train_df['lemmatized'])\n",
    "testX = tfidf_vectorizer.transform(test_df['lemmatized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '00 00',\n",
       " '00 humidity',\n",
       " '00 pm',\n",
       " '00 pm cdt',\n",
       " '00 point',\n",
       " '00 pron',\n",
       " '00 temperature',\n",
       " '000',\n",
       " '000 000',\n",
       " '000 missile',\n",
       " '000 missile bomb',\n",
       " '001',\n",
       " '007',\n",
       " '00am',\n",
       " '00pm',\n",
       " '01',\n",
       " '01 04',\n",
       " '01 2017',\n",
       " '02',\n",
       " '02 04',\n",
       " '02 2017',\n",
       " '03',\n",
       " '03 00',\n",
       " '03 04',\n",
       " '03 17',\n",
       " '03 2017',\n",
       " '03 30',\n",
       " '03 31',\n",
       " '03pm',\n",
       " '04',\n",
       " '04 01',\n",
       " '04 02',\n",
       " '04 03',\n",
       " '04 04',\n",
       " '04 04 2017',\n",
       " '04 05',\n",
       " '04 06',\n",
       " '04 07',\n",
       " '04 08',\n",
       " '04 17',\n",
       " '04 2017',\n",
       " '05',\n",
       " '05 04',\n",
       " '05 2017',\n",
       " '06',\n",
       " '06 04',\n",
       " '06 04 2017',\n",
       " '06 2017',\n",
       " '07',\n",
       " '07 2017',\n",
       " '07 fuckin',\n",
       " '07 fuckin close',\n",
       " '08',\n",
       " '08 2017',\n",
       " '09',\n",
       " '10',\n",
       " '10 00',\n",
       " '10 000',\n",
       " '10 05',\n",
       " '10 10',\n",
       " '10 11',\n",
       " '10 12',\n",
       " '10 15',\n",
       " '10 20',\n",
       " '10 2017',\n",
       " '10 26',\n",
       " '10 30',\n",
       " '10 30 free',\n",
       " '10 32',\n",
       " '10 40',\n",
       " '10 60',\n",
       " '10 60 free',\n",
       " '10 day',\n",
       " '10 day pron',\n",
       " '10 free',\n",
       " '10 free bet',\n",
       " '10 ft',\n",
       " '10 good',\n",
       " '10 million',\n",
       " '10 min',\n",
       " '10 minute',\n",
       " '10 mm',\n",
       " '10 people',\n",
       " '10 people follow',\n",
       " '10 pm',\n",
       " '10 pron',\n",
       " '10 pst',\n",
       " '10 thing',\n",
       " '10 use',\n",
       " '10 use code',\n",
       " '10 week',\n",
       " '10 year',\n",
       " '100',\n",
       " '100 000',\n",
       " '100 15',\n",
       " '100 15 15',\n",
       " '100 amazon',\n",
       " '100 authentic',\n",
       " '100 cotton',\n",
       " '100 follower',\n",
       " '100 free',\n",
       " '100 million',\n",
       " '100 natural',\n",
       " '100 people',\n",
       " '100 pron',\n",
       " '100 year',\n",
       " '100 year ago',\n",
       " '1000',\n",
       " '1000s',\n",
       " '1001',\n",
       " '1001 thing',\n",
       " '1001 thing happen',\n",
       " '100k',\n",
       " '100pcs',\n",
       " '100th',\n",
       " '101',\n",
       " '101 season',\n",
       " '101 season press',\n",
       " '1011',\n",
       " '1011 hpa',\n",
       " '1011 hpa fall',\n",
       " '1012',\n",
       " '1013',\n",
       " '1014',\n",
       " '1015',\n",
       " '102',\n",
       " '1027',\n",
       " '103',\n",
       " '104',\n",
       " '105',\n",
       " '106',\n",
       " '107',\n",
       " '108',\n",
       " '1080p',\n",
       " '109',\n",
       " '10am',\n",
       " '10k',\n",
       " '10pcs',\n",
       " '10pm',\n",
       " '10th',\n",
       " '10x',\n",
       " '11',\n",
       " '11 00',\n",
       " '11 11',\n",
       " '11 12',\n",
       " '11 14',\n",
       " '11 15',\n",
       " '11 16',\n",
       " '11 30',\n",
       " '11 km',\n",
       " '11 km humidity',\n",
       " '11 people',\n",
       " '11 people follow',\n",
       " '11 pron',\n",
       " '11 year',\n",
       " '11 year old',\n",
       " '110',\n",
       " '111',\n",
       " '112',\n",
       " '113',\n",
       " '114',\n",
       " '115',\n",
       " '116',\n",
       " '118',\n",
       " '119',\n",
       " '11am',\n",
       " '11pm',\n",
       " '11th',\n",
       " '12',\n",
       " '12 10',\n",
       " '12 13',\n",
       " '12 14',\n",
       " '12 15',\n",
       " '12 30',\n",
       " '12 hour',\n",
       " '12 inch',\n",
       " '12 mm',\n",
       " '12 month',\n",
       " '12 people',\n",
       " '12 people follow',\n",
       " '12 pron',\n",
       " '12 year',\n",
       " '120',\n",
       " '120 gb',\n",
       " '122s',\n",
       " '122s shy',\n",
       " '122s shy buemi',\n",
       " '123',\n",
       " '125',\n",
       " '128',\n",
       " '128 gb',\n",
       " '12th',\n",
       " '12v',\n",
       " '13',\n",
       " '13 13',\n",
       " '13 14',\n",
       " '13 15',\n",
       " '13 people',\n",
       " '13 people follow',\n",
       " '13 reason',\n",
       " '13 reason fuck',\n",
       " '13 reason fucking',\n",
       " '13 reason netflix',\n",
       " '13 reason pron',\n",
       " '13 reason really',\n",
       " '13 wind',\n",
       " '13 year',\n",
       " '13 year old',\n",
       " '130',\n",
       " '132',\n",
       " '133',\n",
       " '135',\n",
       " '136',\n",
       " '139',\n",
       " '13rw',\n",
       " '13rw horrible',\n",
       " '13rw horrible people',\n",
       " '13th',\n",
       " '14',\n",
       " '14 12',\n",
       " '14 13',\n",
       " '14 14',\n",
       " '14 15',\n",
       " '14 16',\n",
       " '14 day',\n",
       " '14 people',\n",
       " '14 pron',\n",
       " '14 wind',\n",
       " '14 wind mph',\n",
       " '14 year',\n",
       " '14 year old',\n",
       " '140',\n",
       " '146',\n",
       " '147',\n",
       " '14k',\n",
       " '14k gold',\n",
       " '14k yellow',\n",
       " '14k yellow gold',\n",
       " '14th',\n",
       " '14th april',\n",
       " '15',\n",
       " '15 00',\n",
       " '15 12',\n",
       " '15 13',\n",
       " '15 14',\n",
       " '15 15',\n",
       " '15 15 15',\n",
       " '15 16',\n",
       " '15 hour',\n",
       " '15 min',\n",
       " '15 minute',\n",
       " '15 people',\n",
       " '15 people follow',\n",
       " '15 pron',\n",
       " '15 set',\n",
       " '15 tackle',\n",
       " '15 year',\n",
       " '150',\n",
       " '1500',\n",
       " '154',\n",
       " '15am',\n",
       " '15pm',\n",
       " '15th',\n",
       " '16',\n",
       " '16 00',\n",
       " '16 17',\n",
       " '16 18',\n",
       " '16 gb',\n",
       " '16 gb black',\n",
       " '16 gb white',\n",
       " '16 gb wi',\n",
       " '16 year',\n",
       " '16 year old',\n",
       " '160',\n",
       " '160 gb',\n",
       " '162',\n",
       " '16th',\n",
       " '17',\n",
       " '17 00',\n",
       " '17 thing',\n",
       " '17 thing fucking',\n",
       " '17 year',\n",
       " '17 year old',\n",
       " '170',\n",
       " '170330',\n",
       " '170401',\n",
       " '170401 heol',\n",
       " '170401 heol eppi',\n",
       " '170402',\n",
       " '170403',\n",
       " '170405',\n",
       " '170406',\n",
       " '175',\n",
       " '17th',\n",
       " '18',\n",
       " '18 21',\n",
       " '18 miserable',\n",
       " '18 miserable year',\n",
       " '18 pron',\n",
       " '18 year',\n",
       " '18 year old',\n",
       " '180',\n",
       " '1800',\n",
       " '18k',\n",
       " '18th',\n",
       " '18th birthday',\n",
       " '19',\n",
       " '19 year',\n",
       " '190',\n",
       " '1911',\n",
       " '1911 pistol',\n",
       " '1911 pistol thank',\n",
       " '1917',\n",
       " '1960',\n",
       " '1964',\n",
       " '1967',\n",
       " '1968',\n",
       " '1969',\n",
       " '1970',\n",
       " '1971',\n",
       " '1973',\n",
       " '1976',\n",
       " '1979',\n",
       " '1980',\n",
       " '1983',\n",
       " '1984',\n",
       " '1987',\n",
       " '1988',\n",
       " '199',\n",
       " '1990',\n",
       " '1991',\n",
       " '1992',\n",
       " '1994',\n",
       " '1995',\n",
       " '1997',\n",
       " '1998',\n",
       " '1999',\n",
       " '19th',\n",
       " '1b',\n",
       " '1d',\n",
       " '1k',\n",
       " '1pc',\n",
       " '1st',\n",
       " '1st april',\n",
       " '1st day',\n",
       " '1st hr',\n",
       " '1st hr yankee',\n",
       " '1st time',\n",
       " '1st time pron',\n",
       " '20',\n",
       " '20 00',\n",
       " '20 000',\n",
       " '20 free',\n",
       " '20 free spin',\n",
       " '20 fucking',\n",
       " '20 hour',\n",
       " '20 min',\n",
       " '20 minute',\n",
       " '20 people',\n",
       " '20 pron',\n",
       " '20 year',\n",
       " '200',\n",
       " '200 bonus',\n",
       " '200 liar',\n",
       " '200 liar block',\n",
       " '200 welcome',\n",
       " '200 welcome bonus',\n",
       " '2000',\n",
       " '2001',\n",
       " '2001 ludacris',\n",
       " '2001 ludacris bitch',\n",
       " '2002',\n",
       " '2003',\n",
       " '2004',\n",
       " '2004 daddy',\n",
       " '2004 daddy yankee',\n",
       " '2005',\n",
       " '2006',\n",
       " '2007',\n",
       " '2008',\n",
       " '2008 american',\n",
       " '2008 american reject',\n",
       " '2009',\n",
       " '200k',\n",
       " '201',\n",
       " '2010',\n",
       " '2011',\n",
       " '2012',\n",
       " '2013',\n",
       " '2013 pron',\n",
       " '2014',\n",
       " '2014 pron',\n",
       " '2015',\n",
       " '2015 pron',\n",
       " '2016',\n",
       " '2016 17',\n",
       " '2016 pron',\n",
       " '2017',\n",
       " '2017 01',\n",
       " '2017 03',\n",
       " '2017 04',\n",
       " '2017 04 02',\n",
       " '2017 04 04',\n",
       " '2017 04 07',\n",
       " '2017 05',\n",
       " '2017 06',\n",
       " '2017 07',\n",
       " '2017 08',\n",
       " '2017 09',\n",
       " '2017 10',\n",
       " '2017 11',\n",
       " '2017 12',\n",
       " '2017 18',\n",
       " '2017 2018',\n",
       " '2017 badge',\n",
       " '2017 free',\n",
       " '2017 global',\n",
       " '2017 link',\n",
       " '2017 movie',\n",
       " '2017 new',\n",
       " '2017 pron',\n",
       " '2017 season',\n",
       " '2017 watch',\n",
       " '2017 world',\n",
       " '2018',\n",
       " '2019',\n",
       " '202',\n",
       " '2020',\n",
       " '2021',\n",
       " '2022',\n",
       " '2024',\n",
       " '205',\n",
       " '208',\n",
       " '209',\n",
       " '20k',\n",
       " '20th',\n",
       " '21',\n",
       " '21 year',\n",
       " '210',\n",
       " '21st',\n",
       " '21st century',\n",
       " '22',\n",
       " '22 10',\n",
       " '22 fucking',\n",
       " '22 fucking year',\n",
       " '22 year',\n",
       " '225',\n",
       " '226',\n",
       " '226 pron',\n",
       " '226 pron member',\n",
       " '22nd',\n",
       " '22s',\n",
       " '23',\n",
       " '23 travis',\n",
       " '23 travis fucking',\n",
       " '23 year',\n",
       " '23k',\n",
       " '23rd',\n",
       " '24',\n",
       " '24 fuck',\n",
       " '24 hour',\n",
       " '24 hrs',\n",
       " '24 pron',\n",
       " '240',\n",
       " '244',\n",
       " '24k',\n",
       " '24th',\n",
       " '25',\n",
       " '25 000',\n",
       " '25 fuck',\n",
       " '25 fuck million',\n",
       " '25 million',\n",
       " '25 pron',\n",
       " '25 visa',\n",
       " '25 visa gift',\n",
       " '25 year',\n",
       " '250',\n",
       " '250 000',\n",
       " '250 gb',\n",
       " '256',\n",
       " '256 gb',\n",
       " '25th',\n",
       " '26',\n",
       " '262',\n",
       " '27',\n",
       " '270',\n",
       " '27th',\n",
       " '28',\n",
       " '284',\n",
       " '28th',\n",
       " '29',\n",
       " '29 17',\n",
       " '29 44',\n",
       " '29 47',\n",
       " '290',\n",
       " '295',\n",
       " '29th',\n",
       " '2b',\n",
       " '2c',\n",
       " '2d',\n",
       " '2day',\n",
       " '2k',\n",
       " '2k17',\n",
       " '2nd',\n",
       " '2nd time',\n",
       " '2nite',\n",
       " '2pcs',\n",
       " '2x',\n",
       " '30',\n",
       " '30 000',\n",
       " '30 03',\n",
       " '30 17',\n",
       " '30 2017',\n",
       " '30 30',\n",
       " '30 casino',\n",
       " '30 casino bet',\n",
       " '30 day',\n",
       " '30 free',\n",
       " '30 min',\n",
       " '30 minute',\n",
       " '30 pm',\n",
       " '30 pron',\n",
       " '30 sport',\n",
       " '30 sport 30',\n",
       " '30 year',\n",
       " '300',\n",
       " '3000',\n",
       " '303',\n",
       " '30am',\n",
       " '30k',\n",
       " '30k people',\n",
       " '30k people retweete',\n",
       " '30pm',\n",
       " '30th',\n",
       " '30x30',\n",
       " '30x30 trade',\n",
       " '30x30 trade free',\n",
       " '31',\n",
       " '31 17',\n",
       " '31 2017',\n",
       " '31st',\n",
       " '32',\n",
       " '32 gb',\n",
       " '32 gb black',\n",
       " '320',\n",
       " '320 gb',\n",
       " '325',\n",
       " '33',\n",
       " '330',\n",
       " '333',\n",
       " '34',\n",
       " '35',\n",
       " '350',\n",
       " '350 v2',\n",
       " '36',\n",
       " '360',\n",
       " '365',\n",
       " '37',\n",
       " '38',\n",
       " '39',\n",
       " '395',\n",
       " '3d',\n",
       " '3d printing',\n",
       " '3ds',\n",
       " '3ds vote',\n",
       " '3ds vote override',\n",
       " '3ds vote pass',\n",
       " '3ds xl',\n",
       " '3gs',\n",
       " '3k',\n",
       " '3rd',\n",
       " '3rd time',\n",
       " '3x',\n",
       " '3xl',\n",
       " '40',\n",
       " '40 free',\n",
       " '40 pron',\n",
       " '40 year',\n",
       " '400',\n",
       " '400 pron',\n",
       " '401',\n",
       " '404',\n",
       " '41',\n",
       " '42',\n",
       " '420',\n",
       " '43',\n",
       " '436',\n",
       " '44',\n",
       " '45',\n",
       " '45 minute',\n",
       " '45 pm',\n",
       " '45 pron',\n",
       " '450',\n",
       " '450 bonus',\n",
       " '458',\n",
       " '45am',\n",
       " '45pm',\n",
       " '46',\n",
       " '47',\n",
       " '470',\n",
       " '48',\n",
       " '48 hour',\n",
       " '49',\n",
       " '4c',\n",
       " '4k',\n",
       " '4pcs',\n",
       " '4pm',\n",
       " '4s',\n",
       " '4th',\n",
       " '4th 2017',\n",
       " '4u',\n",
       " '4wd',\n",
       " '4x',\n",
       " '50',\n",
       " '50 000',\n",
       " '50 000 missile',\n",
       " '50 07',\n",
       " '50 07 fuckin',\n",
       " '50 credit',\n",
       " '50 cruise',\n",
       " '50 cruise missile',\n",
       " '50 free',\n",
       " '50 mm',\n",
       " '50 pron',\n",
       " '50 tomahawk',\n",
       " '50 year',\n",
       " '500',\n",
       " '500 gb',\n",
       " '5000',\n",
       " '500k',\n",
       " '500px',\n",
       " '50am',\n",
       " '50k',\n",
       " '51',\n",
       " '52',\n",
       " '53',\n",
       " '54',\n",
       " '55',\n",
       " '56',\n",
       " '57',\n",
       " '57 12',\n",
       " '58',\n",
       " '58am',\n",
       " '58pm',\n",
       " '59',\n",
       " '599',\n",
       " '5c',\n",
       " '5d',\n",
       " '5k',\n",
       " '5pm',\n",
       " '5s',\n",
       " '5s 16',\n",
       " '5s 16 gb',\n",
       " '5sos',\n",
       " '5th',\n",
       " '5th anniversary',\n",
       " '5th generation',\n",
       " '5v',\n",
       " '5x',\n",
       " '60',\n",
       " '60 bonus',\n",
       " '60 day',\n",
       " '60 free',\n",
       " '60 free bet',\n",
       " '60 free today',\n",
       " '60 gb',\n",
       " '60 missile',\n",
       " '60 pron',\n",
       " '600',\n",
       " '6000',\n",
       " '61',\n",
       " '62',\n",
       " '63',\n",
       " '64',\n",
       " '64 gb',\n",
       " '65',\n",
       " '66',\n",
       " '666',\n",
       " '67',\n",
       " '68',\n",
       " '69',\n",
       " '6pm',\n",
       " '6s',\n",
       " '6s crystal',\n",
       " '6s crystal case',\n",
       " '6s plus',\n",
       " '6th',\n",
       " '6th april',\n",
       " '6th generation',\n",
       " '6x',\n",
       " '70',\n",
       " '700',\n",
       " '701',\n",
       " '71',\n",
       " '72',\n",
       " '73',\n",
       " '74',\n",
       " '75',\n",
       " '750',\n",
       " '755',\n",
       " '76',\n",
       " '76ers',\n",
       " '77',\n",
       " '777',\n",
       " '78',\n",
       " '79',\n",
       " '7pm',\n",
       " '7s',\n",
       " '7th',\n",
       " '7th april',\n",
       " '80',\n",
       " '800',\n",
       " '808',\n",
       " '80th',\n",
       " '80th anniversary',\n",
       " '80th anniversary make',\n",
       " '81',\n",
       " '811',\n",
       " '82',\n",
       " '83',\n",
       " '84',\n",
       " '840',\n",
       " '85',\n",
       " '855',\n",
       " '86',\n",
       " '87',\n",
       " '88',\n",
       " '888',\n",
       " '89',\n",
       " '8pm',\n",
       " '8th',\n",
       " '8th april',\n",
       " '8th april 2017',\n",
       " '8x10',\n",
       " '90',\n",
       " '90 pron',\n",
       " '900',\n",
       " '91',\n",
       " '91 14',\n",
       " '91 15',\n",
       " '911',\n",
       " '911 dispatch',\n",
       " '911 pron',\n",
       " '911 pron son',\n",
       " '92',\n",
       " '925',\n",
       " '925 silver',\n",
       " '925 silver overlay',\n",
       " '925 sterling',\n",
       " '925 sterling silver',\n",
       " '93',\n",
       " '94',\n",
       " '95',\n",
       " '96',\n",
       " '97',\n",
       " '98',\n",
       " '99',\n",
       " '99 bag',\n",
       " '99 bag skittle',\n",
       " '99 gt',\n",
       " '99 gt free',\n",
       " '99 pron',\n",
       " '999',\n",
       " '999 pron',\n",
       " '999 pron 1st',\n",
       " '9s',\n",
       " '9th',\n",
       " 'TM',\n",
       " 'a1',\n",
       " 'a2',\n",
       " 'a3',\n",
       " 'a4',\n",
       " 'a5',\n",
       " 'a7',\n",
       " 'aa',\n",
       " 'aaa',\n",
       " 'aah',\n",
       " 'aamir',\n",
       " 'aap',\n",
       " 'aaron',\n",
       " 'ab',\n",
       " 'ab pron',\n",
       " 'abandon',\n",
       " 'abandon pron',\n",
       " 'abbey',\n",
       " 'abby',\n",
       " 'abc',\n",
       " 'abc news',\n",
       " 'abc news report',\n",
       " 'abduct',\n",
       " 'ability',\n",
       " 'ability discern',\n",
       " 'ability discern push',\n",
       " 'ability free',\n",
       " 'ability free people',\n",
       " 'able',\n",
       " 'able make',\n",
       " 'able pron',\n",
       " 'abo',\n",
       " 'abo virgo',\n",
       " 'aboard',\n",
       " 'abort',\n",
       " 'abortion',\n",
       " 'abou',\n",
       " 'abril',\n",
       " 'abroad',\n",
       " 'abs',\n",
       " 'absence',\n",
       " 'absolute',\n",
       " 'absolute bad',\n",
       " 'absolutely',\n",
       " 'absolutely fuck',\n",
       " 'absolutely fucking',\n",
       " 'absolutely hate',\n",
       " 'absolutely love',\n",
       " 'absolutely pron',\n",
       " 'absorb',\n",
       " 'abstract',\n",
       " 'absurd',\n",
       " 'abt',\n",
       " 'abt pron',\n",
       " 'abu',\n",
       " 'abundance',\n",
       " 'abuse',\n",
       " 'abuse pron',\n",
       " 'abused',\n",
       " 'abuser',\n",
       " 'abusive',\n",
       " 'ac',\n",
       " 'aca',\n",
       " 'academia',\n",
       " 'academic',\n",
       " 'academy',\n",
       " 'acc',\n",
       " 'acc pron',\n",
       " 'acca',\n",
       " 'accelerate',\n",
       " 'accent',\n",
       " 'accept',\n",
       " 'accept fact',\n",
       " 'accept idiot',\n",
       " 'accept pron',\n",
       " 'accept refugee',\n",
       " 'acceptable',\n",
       " 'acceptance',\n",
       " 'access',\n",
       " 'access pron',\n",
       " 'accessory',\n",
       " 'accident',\n",
       " 'accident pron',\n",
       " 'accidental',\n",
       " 'accidentally',\n",
       " 'accommodation',\n",
       " 'accomplish',\n",
       " 'accomplish pron',\n",
       " 'accomplish pron cancer',\n",
       " 'accomplishment',\n",
       " 'accord',\n",
       " 'accord idiot',\n",
       " 'accord idiot mccain',\n",
       " 'accord pron',\n",
       " 'account',\n",
       " 'account automatically',\n",
       " 'account automatically delete',\n",
       " 'account pron',\n",
       " 'account right',\n",
       " 'account right way',\n",
       " 'account try',\n",
       " 'account try pron',\n",
       " 'accountability',\n",
       " 'accountable',\n",
       " 'accountable pron',\n",
       " 'accountant',\n",
       " 'accs',\n",
       " 'acct',\n",
       " 'accuracy',\n",
       " 'accurate',\n",
       " 'accusation',\n",
       " 'accuse',\n",
       " 'accuse pron',\n",
       " 'accuse trump',\n",
       " 'ace',\n",
       " 'ace luffy',\n",
       " 'ace luffy form',\n",
       " 'ache',\n",
       " 'achieve',\n",
       " 'achieve pron',\n",
       " 'achievement',\n",
       " 'achievement paradise',\n",
       " 'achievement paradise island',\n",
       " 'acid',\n",
       " 'acknowledge',\n",
       " 'acknowledge pron',\n",
       " 'acl',\n",
       " 'acm',\n",
       " 'acm award',\n",
       " 'acmc_clock_euro',\n",
       " 'acne',\n",
       " 'acoustic',\n",
       " 'acquaintance',\n",
       " 'acquire',\n",
       " 'acquisition',\n",
       " 'acre',\n",
       " 'acrylic',\n",
       " 'act',\n",
       " 'act creation',\n",
       " 'act creation today',\n",
       " 'act like',\n",
       " 'act like fucking',\n",
       " 'act like pron',\n",
       " 'act north',\n",
       " 'act north korea',\n",
       " 'act pron',\n",
       " 'act retard',\n",
       " 'action',\n",
       " 'action earn',\n",
       " 'action earn pron',\n",
       " 'action figure',\n",
       " 'action plan',\n",
       " 'action plan base',\n",
       " 'action pron',\n",
       " 'activate',\n",
       " 'activate pron',\n",
       " 'activate pron account',\n",
       " 'active',\n",
       " 'actively',\n",
       " 'activism',\n",
       " 'activist',\n",
       " 'activit',\n",
       " 'activit gemini',\n",
       " 'activity',\n",
       " 'activity calendar',\n",
       " 'activity calendar today',\n",
       " 'actor',\n",
       " 'actor pron',\n",
       " 'actress',\n",
       " 'actual',\n",
       " 'actually',\n",
       " 'actually disgusting',\n",
       " 'actually disgusting minging',\n",
       " 'actually fuck',\n",
       " 'actually fucking',\n",
       " 'actually fucking beast',\n",
       " 'actually good',\n",
       " 'actually make',\n",
       " 'actually pretty',\n",
       " 'actually pron',\n",
       " 'actually want',\n",
       " 'actually work',\n",
       " 'actuellement',\n",
       " 'acutely',\n",
       " 'acutely aware',\n",
       " 'acutely aware pron',\n",
       " 'ad',\n",
       " 'ad libs',\n",
       " 'ad libs pron',\n",
       " 'ad pron',\n",
       " 'adam',\n",
       " 'adam eve',\n",
       " 'adam eve promo',\n",
       " 'adams',\n",
       " 'adapt',\n",
       " 'adapt pron',\n",
       " 'adaptability',\n",
       " 'adaptability useful',\n",
       " 'adaptability useful trait',\n",
       " 'adaptation',\n",
       " 'adapter',\n",
       " 'add',\n",
       " 'add new',\n",
       " 'add new feature',\n",
       " 'add pron',\n",
       " 'add pron closet',\n",
       " 'add pron snapchat',\n",
       " 'add snapchat',\n",
       " 'add snapchat lustysnap',\n",
       " 'add video',\n",
       " 'add video playlist',\n",
       " 'addict',\n",
       " 'addiction',\n",
       " 'addison',\n",
       " 'addition',\n",
       " 'addition pron',\n",
       " 'additional',\n",
       " 'address',\n",
       " 'address gt',\n",
       " 'address gt gt',\n",
       " 'address pron',\n",
       " 'addy',\n",
       " 'adele',\n",
       " 'adhd',\n",
       " 'adida',\n",
       " 'adidas',\n",
       " 'adidas yeezy',\n",
       " 'adjust',\n",
       " 'adjust pron',\n",
       " 'adjustable',\n",
       " 'admin',\n",
       " 'administration',\n",
       " ...]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=42, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_classifier = RandomForestClassifier(n_estimators = 100, criterion = 'gini', random_state = 42) #entropy\n",
    "rf_classifier.fit(trainX, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98     43003\n",
      "           1       0.96      0.93      0.94     11213\n",
      "           2       0.97      0.99      0.98     21781\n",
      "           3       0.96      0.90      0.93      3999\n",
      "\n",
      "    accuracy                           0.97     79996\n",
      "   macro avg       0.97      0.95      0.96     79996\n",
      "weighted avg       0.97      0.97      0.97     79996\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predY = rf_classifier.predict(trainX)\n",
    "print(classification_report(trainY, predY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.92      0.85     10848\n",
      "           1       0.58      0.29      0.39      2817\n",
      "           2       0.85      0.91      0.88      5369\n",
      "           3       0.61      0.21      0.31       966\n",
      "\n",
      "    accuracy                           0.79     20000\n",
      "   macro avg       0.71      0.58      0.61     20000\n",
      "weighted avg       0.77      0.79      0.77     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predY = rf_classifier.predict(testX)\n",
    "print(classification_report(testY, predY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsOneClassifier(estimator=LogisticRegression(C=0.1, class_weight='balanced',\n",
       "                                                dual=False, fit_intercept=True,\n",
       "                                                intercept_scaling=1,\n",
       "                                                l1_ratio=None, max_iter=2000,\n",
       "                                                multi_class='warn', n_jobs=None,\n",
       "                                                penalty='l2', random_state=None,\n",
       "                                                solver='warn', tol=0.0001,\n",
       "                                                verbose=0, warm_start=False),\n",
       "                   n_jobs=-1)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_classifier = OneVsOneClassifier(LogisticRegression(class_weight='balanced', C=0.1, max_iter=2000), n_jobs=-1)\n",
    "lr_classifier.fit(trainX, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.71      0.79     43003\n",
      "           1       0.44      0.75      0.55     11213\n",
      "           2       0.91      0.78      0.84     21781\n",
      "           3       0.35      0.64      0.45      3999\n",
      "\n",
      "    accuracy                           0.73     79996\n",
      "   macro avg       0.64      0.72      0.66     79996\n",
      "weighted avg       0.80      0.73      0.75     79996\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predY = lr_classifier.predict(trainX)\n",
    "print(classification_report(trainY, predY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.70      0.78     10848\n",
      "           1       0.42      0.73      0.53      2817\n",
      "           2       0.91      0.77      0.83      5369\n",
      "           3       0.31      0.57      0.40       966\n",
      "\n",
      "    accuracy                           0.72     20000\n",
      "   macro avg       0.63      0.69      0.64     20000\n",
      "weighted avg       0.79      0.72      0.74     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predY = lr_classifier.predict(testX)\n",
    "print(classification_report(testY, predY))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
